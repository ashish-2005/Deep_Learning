{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c521c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/IRIS.csv')\n",
    "data = data.sample(frac=1)\n",
    "data.replace({'species':{list(data['species'].unique())[i]:i for i in range(0,3)}},inplace=True)\n",
    "data_train = data.iloc[:75,:]\n",
    "data_test = data.iloc[75:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b0e1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(y):\n",
    "    number_of_class = len(np.unique(y))\n",
    "    return np.eye(number_of_class)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "827c303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (data_train.drop('species',axis=1).to_numpy()).T\n",
    "Y = (one_hot_encoder(data_train['species'].to_numpy())).T\n",
    "X_test = (data_test.drop('species',axis=1).to_numpy()).T\n",
    "Y_test = (one_hot_encoder(data_test['species'].to_numpy())).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc5a205",
   "metadata": {},
   "source": [
    "***create_network(input_size,output_size,hidden_layers,layer_size)***\\\n",
    "\\\n",
    "*input_size* : number of features in dataset\\\n",
    "*output_size* : 1 in case of regression and K in case of k classes\\\n",
    "*hidden_layers* : number of hidden layers\\\n",
    "*layer_size* : size(number of neuron) in each layer\\\n",
    "\\\n",
    "aim is to generate random initial weight and bias for layers\\\n",
    "returns a dictionary *parameters*\\\n",
    "where $(W_{i})_{mxn}$ $m$ is size of the $i^{th}$ layer and $n$ is size of $(i-1)^{th}$ layer\\\n",
    "and $(b_{i})_{mx1}$ $m$ is size of the $i^{th}$ layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ee8d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating random initial parameter ie. weights and bias for different layer\n",
    "def create_network(input_size,output_size,hidden_layers,layer_size):\n",
    "    parameters = {}\n",
    "    np.random.seed(42)\n",
    "    sizes = [input_size]+layer_size+[output_size]\n",
    "    for i in range(1,hidden_layers+2):\n",
    "        parameters['w'+str(i)] = np.random.randn(sizes[i],sizes[i-1])\n",
    "        parameters['b'+str(i)] = np.random.randn(sizes[i],1)\n",
    "        #print('creation b size',parameters['b'+str(i)].shape)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7fbb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just some activation function , its derevative and output fuction respectively\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def grad_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "def softmax(x):\n",
    "    n = np.exp(x)\n",
    "    d = np.sum(n,axis=0)\n",
    "    return (n/d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f63bb",
   "metadata": {},
   "source": [
    "***feed_forward(X,hidden_layers,parameters)***\\\n",
    "\\\n",
    "*X* : data matrix\\\n",
    "*hidden_layers* : number of hidden layers\\\n",
    "*parameters* : dictioanry of weight and bias\\\n",
    "\\\n",
    "return dictionary A and H\\\n",
    "where $a_{i}$ in A is the pre-activation result of $i^{th}$ layer\\\n",
    "and $h_{i}$ in H is the activation result of $i^{th}$ layer\\\n",
    "\\\n",
    "also return $\\hat y$ which is result matrix of neural network (just a vector in case of regression)\\\n",
    "in $\\hat y$ each column correspond to one data point and each row is *probability* of that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2853c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to genrate end-result and the intermidiate a and h\n",
    "def feed_forward(x,hidden_layers,parameters):\n",
    "    A = {}\n",
    "    H = {'h0':x}\n",
    "    \n",
    "    for i in range(1,hidden_layers+2):\n",
    "        #print(i,parameters['w'+str(i)].shape, H['h'+str(i-1)].shape)\n",
    "        A['a'+str(i)] = np.dot(parameters['w'+str(i)], H['h'+str(i-1)]) + parameters['b'+str(i)]\n",
    "        #print(A['a1'].shape)\n",
    "        if i != hidden_layers+1:\n",
    "            H['h'+str(i)] = sigmoid(A['a'+str(i)])\n",
    "        else:\n",
    "            H['h'+str(i)] = softmax(A['a'+str(i)])\n",
    "    y_hat = H['h'+str(hidden_layers+1)]\n",
    "    \n",
    "    return A,H,y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990602a",
   "metadata": {},
   "source": [
    "***back_propogate(A,H,y,y_hat,parameters,hidden_layers,input_size)***\\\n",
    "\\\n",
    "*A , H , y , y_hat , parameters , hidden_layers , input_size* : All have usual meaning as above\\\n",
    "\\\n",
    "return a dictionary *gradients*\\\n",
    "where $dW_{i}$ is gradient of loss function with respect to weight of layer $i$\\\n",
    "similar for $db_{i},da_{i},dh_{i}$ are gradient of loss function w.r.t bias,pre-activation and activation of $i^{th}$ layer respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2cce20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propogate(A,H,y,y_hat,parameters,hidden_layers,input_size):\n",
    "    gradients = {}\n",
    "    A['a0'] = np.zeros((input_size,1))\n",
    "    gradients['da'+str(hidden_layers+1)] = y_hat-y\n",
    "    #print(gradients['da'+str(hidden_layers+1)].shape)\n",
    "    for i in range(hidden_layers+1,0,-1):\n",
    "        gradients[\"dw\" + str(i)] = np.dot(gradients[\"da\" + str(i)], (H[\"h\" + str(i-1)]).T)\n",
    "        gradients[\"db\" + str(i)] = gradients[\"da\" + str(i)]\n",
    "        gradients[\"dh\" + str(i-1)] = np.dot((parameters[\"w\" + str(i)]).T, \n",
    "                                              gradients[\"da\" + str(i)])\n",
    "        derv = grad_sigmoid(A[\"a\" + str(i-1)])\n",
    "        gradients['da'+str(i-1)] = gradients['dh'+str(i-1)]*derv\n",
    "        #print( gradients[\"db\" + str(i)].shape)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a1bf7",
   "metadata": {},
   "source": [
    "below cell is the most basic implementation of gradient decent\n",
    "* first we calculated the A , H and y_hat with feed_forward function\n",
    "* then find gradient of loss function w.r.t weights an bias (as our network output only depend on weights and bias {we calculate da and dh as it required to find dW and db})\n",
    "* then update the weights and bias in parameter ie. decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4d861b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding result with best/post-training parameters\n",
    "def accuracy(decented_param):\n",
    "    A,H,y_hat = feed_forward(X_test,3,decented_param)\n",
    "\n",
    "    y_pred = np.argmax(y_hat,axis=0)\n",
    "    y_true = data_test.replace(\n",
    "        {'species':{list(data_test['species'].unique())[i]:i for i in range(0,3)}}\n",
    "    ).to_numpy()[:,4]\n",
    "    c = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            c+=1\n",
    "    print(f'{c} correct out of {len(y_true)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a51ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 correct out of 75\n",
      "34 correct out of 75\n",
      "34 correct out of 75\n"
     ]
    }
   ],
   "source": [
    "p = create_network(4,3,3,[5,5,5])\n",
    "for i in range(1,1000000,100000):\n",
    "    for K in range(i):\n",
    "        A,H,y_hat = feed_forward(X,3,p)\n",
    "        #print('a1 shape',A['a1'].shape)\n",
    "        gradients = back_propogate(A,H,Y,y_hat,p,3,4)\n",
    "        for i in range(1,5,1):\n",
    "            p['w'+str(i)] = p['w'+str(i)] - gradients['dw'+str(i)]\n",
    "            p['b'+str(i)] = p['b'+str(i)] - gradients['db'+str(i)]\n",
    "        #print(p['b'+str(i)].shape)\n",
    "    accuracy(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc39807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
