{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61654c67",
   "metadata": {},
   "source": [
    "### Biological Analogy for Deep Learning\n",
    "* In deep learning all we do is to try to mimic our brain \n",
    "* So brain itself uses muliple layer of neurons to perform some complex task , and thats exactly what we gonna do.\n",
    "* We gonna replace neuron with artificial-neuron which is digital representation of a biological nerve-cell(neuron)<br>\n",
    "<img src='https://miro.medium.com/max/1400/1*hkYlTODpjJgo32DoCOWN5w.webp' width=45%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dbdb9c",
   "metadata": {},
   "source": [
    "### Different Kinds of Artificial-neurons\n",
    "* **McCulloch Pitts Neuron** : These neuron are generally used for boolean functions(takes input as boolean and return result in boolean) which are linearly separable , ie. these exist a line(plane) such that all input that produce 1 are on one side and those produce 0 are on other side <img src='https://miro.medium.com/max/640/1*fDHlg9iNo0LLK4czQqqO9A.webp' width=25%> \n",
    "the way this neuron work is that it will sum up all the inputs and if result is equal to or greater than certain threshold it will return 1 and else 0\n",
    "* **Perceptrons** : It is also used for linearly separable data but more complex than MP neuron as we added weights and bias and also now input need not to be boolean it can be any real number, and output would be boolean.\n",
    "<img src='https://static.javatpoint.com/tutorial/machine-learning/images/perceptron-in-machine-learning2.png' width=35%> the $x_{0}=1$ and $w_{0}=θ$(bias/threshold)<img src='https://i.ibb.co/qWBszQh/4.png' width=40%>**Training of perceptron** : In training our main aim is to find vector w(weights) which correctly classify each instance. the way its done is :-\n",
    "    * If instance belong to class 1 but $w^{T}x < 0$ then we update w with w+x\n",
    "    * If instance belong to class 0 but $w^{T}x ≥ 0$ then we update w with w-x\n",
    "    * And repeat it untill we achive convergence ie. we correctly classify all (training-)instances , and convergence will definitely be achived given that data is linearly separable.\n",
    "* **Sigmoid Neuron** : Perceptron do hard classification ie. it only take its threshold into refrence while genrating output. To overcome this issue sigmoid neurons were introduced which are perceptron but uses sigmoid for output .<br>one kind of sigmoid function is logistic function ie. $σ(t) = $$1 \\over 1 + e^{-t}$ it give smoother , continuous and diffrentiable curve unlike standard perceptron<br>\n",
    "it also return result in range [0,1] hence, can be treated as probability which further help with soft classification\n",
    "<img src='https://i.ibb.co/K7kGKxK/1.png' width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee38fc",
   "metadata": {},
   "source": [
    "#### MultiLayer Network of perceptron / MultiLayer Perceptron (MLP)\n",
    "* A single perceptron is unable to classify non-linearly separable data but a multilayerd network of perceptron is capable.\n",
    "* The simplest but computational expensive way is to take a hidden layer with $2^{n}$ number of perceptron where $n$ is number of input-features , and find suitable weight for all $2^{n}$ perceptron to produce an output with output layer\n",
    "<img src='https://www.allaboutcircuits.com/uploads/articles/an-introduction-to-training-theory-for-neural-networks_rk_aac_image2.jpg' width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d5b52",
   "metadata": {},
   "source": [
    "#### Representing any arbitrary function with MultiLayerd Network of sigmoid neurons\n",
    "***\n",
    "* An arbitrary function can be approximated with help of several \"tower\"(sigmoid neuron network) function , and more the towers better the approximation \n",
    "\n",
    "<img src='https://i.ibb.co/TM78GjW/5.png' width=30%>\n",
    "\n",
    "* So to create a tower for single input we would be needing 2 sigmoid neuron with different $w$ and $b$ (high $w$ to get something like step-function)then subtracting those to get a tower\n",
    "\n",
    "<img src='https://miro.medium.com/max/1100/1*lqT5YgwP3hXdHnOuXgTu3g.webp' width=50%>\n",
    "\n",
    "and now if we take enough of these tower(network of 2 sigmoid neuron) we can approximate any function with single input\n",
    "* To approximate a function with 2 input-feature we would be needing a 2D tower and take enough of these 2D towers to get good approximation of function\n",
    "    * We will create two sigmoid neuron with $w_{1}=0$ so it remain constant for $x_{1}$ feature and set $w_{2}$ to high value to get a 2D step function then subtract those to get tower\n",
    "    * And reapeat , but with $w_{2}=0$ and $w_{1}$ high , and add new tower with old one to get a 2D tower but with different levels\n",
    "    * Then use another neuron to level down the tower , and we will end up with a 2D tower which can further be used for approximation\n",
    "    \n",
    "<img src='https://miro.medium.com/max/828/1*xUGfnzOlmOLyIILnTSBorQ.webp' width=50%>\n",
    "(in above figure w is not too high hence some curvy towers)\n",
    "\n",
    "* Now we can approximate any function with $n$ features , all we need is $2^{n}$ numbers of sigmoid neurons to generate a tower , and use enough of towers to achive desired precision .\n",
    "\n",
    "<br>\n",
    "\n",
    "This is stated that , its not that only solution but this confirms that there exists a soultion and our learning/training algorithm can converge at a solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494c9a9",
   "metadata": {},
   "source": [
    "#### FeedForward Neural Networks \n",
    "* FeedForward Neural network is fancy name of multilayered network of neuron\n",
    "\n",
    "<img src='https://i.ibb.co/xjSYL7p/7.png' width=30%>\n",
    "\n",
    "* Each layer is consist of pre-activation(aggrigation) and a activation function\n",
    "* Pre-activation at layer i is given by $$a_{i} = b_{i} + W_{i}h_{i-1}$$\n",
    "activation function at layer i is $$h_{i} = g(a_{i})$$\n",
    "and activation function for output layer which is also the output function itself is $$f(x) = h_{L} = O(a_{L})$$ \n",
    "* hence , we can represent whole model as a function \n",
    "$$y_{i} = f(x_{i}) = O(W_{3}g(W_{2}g(W_{1}x_{i}+b_{1})+b_{2})+b_{3})$$\n",
    "above function is representation of a model/neural network with 3 hidden layers , with g(x) as activation function and O(x) as ouput function\n",
    "\n",
    "<br>\n",
    "\n",
    "##### this function $O(x)$ changes with task at hand\n",
    "Most commonly for regression task output function would be a linear layer $O(a_{L}) = W_{o}a_{L}+b_{0}$ with objective/loss function like MSE <br><br>\n",
    "and that of classification output function, it can be softmax function with objective function as cross entropy ie. $L(\\theta) = - \\sum_{c=1}^{k} y_{c} log(\\hat{y_{c}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a84d0",
   "metadata": {},
   "source": [
    "### BackPropogation \n",
    "* It is the method of finding gradient of different units involved in neural network which can further be used for gradient decent \n",
    "\n",
    "#### Type of gradient descent\n",
    "* Vanilla Gradient descent\n",
    "* Momentum based Gradient descent\n",
    "* Nesterov Accelerated Gradient Descent\n",
    "* Stochastic And Mini-Batch Gradient Descent\n",
    "    * Then we can add momentum and Nestrov acceleration to mini-batch for faster optimization \n",
    "* Gradient Descent with adaptive learning\n",
    "    * Adagrad\n",
    "    * Adam (can be used as default in a lot of real-world applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb5fb3",
   "metadata": {},
   "source": [
    "### AutoEncoders\n",
    "* These are special kind of feed-forward neural network use for unsupervised learning which ecode a n-dimensional data to a d-dimensions and again decode it from d to n dimensions\n",
    "    * If $n>d$ these kind of encoders are called under-complete autoencoders\n",
    "    * If $n<d$ these kind of encoders are called over-complete autoencoders\n",
    "* They have a great application in extracting noise , recognizing relevant feature and detecting anomalies\n",
    "\n",
    "***Denoising Autoencoders***\n",
    "* It is a over-complete autoencoder which is robust to the noise in input data by adding regulerization in autoencoder\n",
    "* In these we intentionally introduce noise or corruption in the training data then optimise the network so it can be robust to the further inputs with noise (More the noise more it would be dependent on the immediate features)\n",
    "* It can be used for classifing noisy image , extracting important features etc\n",
    "\n",
    "***Sparse Autoencoder***\n",
    "* It is another type of regularized autoencoder , with the idea to keep the neurons mostly inactive or close to 0\n",
    "\n",
    "***Contractive Autoencoder***\n",
    "* Its also a regularized autoencoder , and the idea here it to regulerize by contradicting the aim , the regulerizing term get minimize when the $h$ is not sensitive to input which is contradicting\n",
    "\n",
    "<img src='https://i.ibb.co/HhkNhDC/8.png' width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b92a1d",
   "metadata": {},
   "source": [
    "### Regulerization\n",
    "* ***L2 regulerization :*** In this we add regulerization term which "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
