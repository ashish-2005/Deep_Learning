{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61654c67",
   "metadata": {},
   "source": [
    "### Biological Analogy for Deep Learning\n",
    "* In deep learning all we do is to try to mimic our brain \n",
    "* So brain itself uses muliple layer of neurons to perform some complex task , and thats exactly what we gonna do.\n",
    "* We gonna replace neuron with artificial-neuron which is digital representation of a biological nerve-cell(neuron)<br>\n",
    "<img src='https://miro.medium.com/max/1400/1*hkYlTODpjJgo32DoCOWN5w.webp' width=45%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dbdb9c",
   "metadata": {},
   "source": [
    "### Different Kinds of Artificial-neurons\n",
    "* **McCulloch Pitts Neuron** : These neuron are generally used for boolean functions(takes input as boolean and return result in boolean) which are linearly separable , ie. these exist a line(plane) such that all input that produce 1 are on one side and those produce 0 are on other side <img src='https://miro.medium.com/max/640/1*fDHlg9iNo0LLK4czQqqO9A.webp' width=25%> \n",
    "the way this neuron work is that it will sum up all the inputs and if result is equal to or greater than certain threshold it will return 1 and else 0\n",
    "* **Perceptrons** : It is also used for linearly separable data but more complex than MP neuron as we added weights and bias and also now input need not to be boolean it can be any real number, and output would be boolean.\n",
    "<img src='https://static.javatpoint.com/tutorial/machine-learning/images/perceptron-in-machine-learning2.png' width=35%> the $x_{0}=1$ and $w_{0}=θ$(bias/threshold)<img src='https://i.ibb.co/qWBszQh/4.png' width=40%>**Training of perceptron** : In training our main aim is to find vector w(weights) which correctly classify each instance. the way its done is :-\n",
    "    * If instance belong to class 1 but $w^{T}x < 0$ then we update w with w+x\n",
    "    * If instance belong to class 0 but $w^{T}x ≥ 0$ then we update w with w-x\n",
    "    * And repeat it untill we achive convergence ie. we correctly classify all (training-)instances , and convergence will definitely be achived given that data is linearly separable.\n",
    "* **Sigmoid Neuron** : Perceptron do hard classification ie. it only take its threshold into refrence while genrating output. To overcome this issue sigmoid neurons were introduced which are perceptron but uses sigmoid for output .<br>one kind of sigmoid function is logistic function ie. $σ(t) = $$1 \\over 1 + e^{-t}$ it give smoother , continuous and diffrentiable curve unlike standard perceptron<br>\n",
    "it also return result in range [0,1] hence, can be treated as probability which further help with soft classification\n",
    "<img src='https://i.ibb.co/K7kGKxK/1.png' width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee38fc",
   "metadata": {},
   "source": [
    "#### MultiLayer Network of perceptron / MultiLayer Perceptron (MLP)\n",
    "* A single perceptron is unable to classify non-linearly separable data but a multilayerd network of perceptron is capable.\n",
    "* The simplest but computational expensive way is to take a hidden layer with $2^{n}$ number of perceptron where $n$ is number of input-features , and find suitable weight for all $2^{n}$ perceptron to produce an output with output layer\n",
    "<img src='https://www.allaboutcircuits.com/uploads/articles/an-introduction-to-training-theory-for-neural-networks_rk_aac_image2.jpg' width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d5b52",
   "metadata": {},
   "source": [
    "#### Representing any arbitrary function with MultiLayerd Network of sigmoid neurons\n",
    "***\n",
    "* An arbitrary function can be approximated with help of several \"tower\"(sigmoid neuron network) function , and more the towers better the approximation \n",
    "\n",
    "<img src='https://i.ibb.co/TM78GjW/5.png' width=30%>\n",
    "\n",
    "* So to create a tower for single input we would be needing 2 sigmoid neuron with different $w$ and $b$ (high $w$ to get something like step-function)then subtracting those to get a tower\n",
    "\n",
    "<img src='https://miro.medium.com/max/1100/1*lqT5YgwP3hXdHnOuXgTu3g.webp' width=50%>\n",
    "\n",
    "and now if we take enough of these tower(network of 2 sigmoid neuron) we can approximate any function with single input\n",
    "* To approximate a function with 2 input-feature we would be needing a 2D tower and take enough of these 2D towers to get good approximation of function\n",
    "    * We will create two sigmoid neuron with $w_{1}=0$ so it remain constant for $x_{1}$ feature and set $w_{2}$ to high value to get a 2D step function then subtract those to get tower\n",
    "    * And reapeat , but with $w_{2}=0$ and $w_{1}$ high , and add new tower with old one to get a 2D tower but with different levels\n",
    "    * Then use another neuron to level down the tower , and we will end up with a 2D tower which can further be used for approximation\n",
    "    \n",
    "<img src='https://miro.medium.com/max/828/1*xUGfnzOlmOLyIILnTSBorQ.webp' width=50%>\n",
    "(in above figure w is not too high hence some curvy towers)\n",
    "\n",
    "* Now we can approximate any function with $n$ features , all we need is $2^{n}$ numbers of sigmoid neurons to generate a tower , and use enough of towers to achive desired precision .\n",
    "\n",
    "<br>\n",
    "\n",
    "This is stated that , its not that only solution but this confirms that there exists a soultion and our learning/training algorithm can converge at a solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494c9a9",
   "metadata": {},
   "source": [
    "#### FeedForward Neural Networks \n",
    "* FeedForward Neural network is fancy name of multilayered network of neuron\n",
    "\n",
    "<img src='https://i.ibb.co/xjSYL7p/7.png' width=30%>\n",
    "\n",
    "* Each layer is consist of pre-activation(aggrigation) and a activation function\n",
    "* Pre-activation at layer i is given by $$a_{i} = b_{i} + W_{i}h_{i-1}$$\n",
    "activation function at layer i is $$h_{i} = g(a_{i})$$\n",
    "and activation function for output layer which is also the output function itself is $$f(x) = h_{L} = O(a_{L})$$ \n",
    "* hence , we can represent whole model as a function \n",
    "$$y_{i} = f(x_{i}) = O(W_{3}g(W_{2}g(W_{1}x_{i}+b_{1})+b_{2})+b_{3})$$\n",
    "above function is representation of a model/neural network with 3 hidden layers , with g(x) as activation function and O(x) as ouput function\n",
    "\n",
    "<br>\n",
    "\n",
    "##### this function $O(x)$ changes with task at hand\n",
    "Most commonly for regression task output function would be a linear layer $O(a_{L}) = W_{o}a_{L}+b_{0}$ with objective/loss function like MSE <br><br>\n",
    "and that of classification output function, it can be softmax function with objective function as cross entropy ie. $L(\\theta) = - \\sum_{c=1}^{k} y_{c} log(\\hat{y_{c}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a84d0",
   "metadata": {},
   "source": [
    "### BackPropogation \n",
    "* It is the method of finding gradient of different units involved in neural network which can further be used for gradient decent \n",
    "\n",
    "#### Type of gradient descent\n",
    "* Vanilla Gradient descent\n",
    "* Momentum based Gradient descent\n",
    "* Nesterov Accelerated Gradient Descent\n",
    "* Stochastic And Mini-Batch Gradient Descent\n",
    "    * Then we can add momentum and Nestrov acceleration to mini-batch for faster optimization \n",
    "* Gradient Descent with adaptive learning\n",
    "    * Adagrad\n",
    "    * Adam (can be used as default in a lot of real-world applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb5fb3",
   "metadata": {},
   "source": [
    "### AutoEncoders\n",
    "* These are special kind of feed-forward neural network use for unsupervised learning which ecode a n-dimensional data to a d-dimensions and again decode it from d to n dimensions\n",
    "    * If $n>d$ these kind of encoders are called under-complete autoencoders\n",
    "    * If $n<d$ these kind of encoders are called over-complete autoencoders\n",
    "* They have a great application in extracting noise , recognizing relevant feature and detecting anomalies\n",
    "\n",
    "***Denoising Autoencoders***\n",
    "* It is a over-complete autoencoder which is robust to the noise in input data by adding regulerization in autoencoder\n",
    "* In these we intentionally introduce noise or corruption in the training data then optimise the network so it can be robust to the further inputs with noise (More the noise more it would be dependent on the immediate features)\n",
    "* It can be used for classifing noisy image , extracting important features etc\n",
    "\n",
    "***Sparse Autoencoder***\n",
    "* It is another type of regularized autoencoder , with the idea to keep the neurons mostly inactive or close to 0\n",
    "\n",
    "***Contractive Autoencoder***\n",
    "* Its also a regularized autoencoder , and the idea here it to regulerize by contradicting the aim , the regulerizing term get minimize when the $h$ is not sensitive to input which is contradicting\n",
    "\n",
    "<img src='https://i.ibb.co/HhkNhDC/8.png' width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285c97c",
   "metadata": {},
   "source": [
    "### Regulerization\n",
    "* ***L2 regulerization :*** In this we add regulerization term to the loss function which is a vector of parameters(weights and bias) and when we minimize loss function , the parameter vector also get minimize hence the parameters stays low \n",
    "$$\\Omega(\\theta) = \\alpha \\sum_{i=1}^{M}(\\theta_{i})^{2}$$\n",
    "* ***Dataset augmentation :*** It is the process of genrating more datapoints from the existing datapoint in this way we can get a diverse data so the network won't overfit over a perticular set of data, work well with image-classification/object-detection and speech . For example, if we train network with only vertical and centered digit it might fail to recognise some tilted or not-centered digit , so to tackel it we can add new datapoints(images) with some rotation and different allignment so our network generalize better and be robust to little-noise in testing datapoints\n",
    "\n",
    "* ***Parameter sharing and tying :*** Parameter-sharing is used in CNNs , Same filter is appplied at different position or same weight matrix used for different input neuron . Parameter-tying typically used in Autoencoders in this the encoder and decoder weights are tied(same)\n",
    "\n",
    "* ***Adding Noise to Input :*** In this we intentionally add noise/corruption to the training dataset to make our network generlize better , it is equivelent to l2 norm panelty , used in over-complete autoencoders\n",
    "\n",
    "* ***Adding Noise to output :*** In this we add noise to output just to ensure our netwrok don't overfit .\n",
    "\n",
    "* ***Early-Stopping :*** We keep track of our validation error and stop training once our validation error start increasing or constant as it tells us that our network had started overfitting just to reduce training error\n",
    "\n",
    "* ***Dropout :*** It is ensemble method of neural network , but insted of taking multipe network , training them then feedforwarding them then do voting , *we just train same neural network in multiple ways*\n",
    "    * We randomly drop neurons from hidden layers and then train it , so neurons don't co-adapt(ie. perticular neurons start focusing on perticular features insted of considring whole data-point OR perticular neuron adjust itself just to correct another neuron mistake)\n",
    "    * After training we can use whole network just scale the output of each neuron by the fraction of time it was trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094eaad1",
   "metadata": {},
   "source": [
    "### Operations for better result\n",
    "* ***Unsupervised pre-training :*** It is the technique of pre-training network so each layer is representation of datapoint , just like hidden-layer of autoencoder . This will make sure backpropogation will converge at optimum solution. This technique lead to the devolopment of better optimization and regularization algorithms\n",
    "* ***Better Activation function*** : Sigmoid has its cons , like it suffers form vanishing gradients as it gets saturated and its not zero-centered\n",
    "    * tanh : Its a zero-centered form of sigmoid , but still faces vanishing gradients , generally used in LSTMs and RNNs\n",
    "    * ReLU : Rectified Linear Unit max(0,x) , its defined for positive part and 0 for negetive part hence can result in some dead neurons . generally used in CNNs\n",
    "        * Leaky ReLU : $max(0.01x,x)$\n",
    "        * exponential RelU : $max(e^{x},x)$\n",
    "* ***Better Initialization :*** Most of time if we take random initial weights which are either too large or too small it was resulting in loss of variance which sabotaging the gradients hence making deficult to reach optimal solution. So solution for it we divede the random initial parameters with square root of number of inputs called Xavier initialization. \n",
    "```python\n",
    "W = np.random.randn(n_input,n_output)/np.sqrt(n_input)\n",
    "W = np.random.randn(n_input,n_output)/np.sqrt(n_input/2) #in case of ReLU\n",
    "```\n",
    "* ***Batch Normalization :*** It is a technique for training very deep neural networks that normalizes the contributions to a layer for every mini-batch. This has the impact of settling the learning process and drastically decreasing the number of training epochs required to train deep neural networks. (A Batch-normalization layer can be added after evey layer)\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "*The most prevelent combination would be **Adam** for optimization , **ReLU** for activation , **Xavier** for initialization , **Dropout** for regulerization and **Batch-nomalization** for both regulerization and initialization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0ede4",
   "metadata": {},
   "source": [
    "### Word to Vector\n",
    "* ***Vocabulary*** : It is the set of unique words present in the corpus which is a set of sentances or small phrases\n",
    "* ***Distributive-representation*** : A co-occurrance matrix is a VxV(V=len(Vocabulary)) matrix which captures the number of time word appears in context of another word. Still it would be a huge matrix as V can be as large as 13 million(google 1T dataset size) , so we can use SVD .\n",
    "* ***Continuous Bag of Words model*** : we try to predict centre word by summing vectors of surrounding words . We begin with small random initialization of word vectors. Our predictive model learns the vectors by minimizing the loss function.\n",
    "* ***Skip-gram :*** The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word).Used more commonly then CBOW. In this we change softmax function with other function as softmax of a huge matrix would be computational expensive , like contrastive estimation or hierarchial softmax\n",
    "* ***GloVe*** : Its the hybrid of count based(Distributive-representation) and prediction based(CBOW/skip-gram) word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1e16f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
